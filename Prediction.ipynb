{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "from statistics import mode \n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# TFIDF\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy_user(model_name):\n",
    "    correct_pred = 0\n",
    "    for id_num in ids_test:\n",
    "        c = Counter(df_test[df_test[\"id\"] == id_num][\"predictions\"])  \n",
    "        mode_count = max(c.values())\n",
    "        mode = {key for key, count in c.items() if count == mode_count}\n",
    "        pred = next(x for x in df_test[df_test[\"id\"] == id_num][\"predictions\"] if x in mode)\n",
    "        #real = mode(df_test[df_test[\"id\"] == id_num][\"location\"])\n",
    "        lst  =(list(df_test[df_test[\"id\"] == id_num][\"location\"]))\n",
    "        real = max(set(lst), key=lst.count)\n",
    "        if real == pred:\n",
    "            correct_pred = correct_pred + 1\n",
    "\n",
    "    print(\"Accuracy for {}: {}\".format(model_name, correct_pred/len(ids_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv(\"tweet_dataset_processed_ext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle all unique ids\n",
    "df_processed = df_processed.dropna()\n",
    "ids = list(df_processed[\"id\"].unique())\n",
    "ids_shuffled = random.sample(ids, len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "# Split ids in train/test 0.9/0.1 approximately\n",
    "# We should predict multiple times with randomly shuffled data to avoid overfitting\n",
    "print(len(df_processed[\"id\"].unique()))\n",
    "ids_train = ids_shuffled[:500]\n",
    "ids_test = ids_shuffled[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train and test\n",
    "df_train = df_processed[df_processed['id'].isin(ids_train)]\n",
    "df_test = df_processed[df_processed['id'].isin(ids_test)]\n",
    "\n",
    "X_train = df_train[\"tweet\"]\n",
    "X_test = df_test[\"tweet\"]\n",
    "y_train = df_train[\"location\"]\n",
    "y_test = df_test[\"location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform tweets with TFIDF vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest: 0.4406779661016949\n",
      "Accuracy for Linear SVC: 0.576271186440678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Reg: 0.5084745762711864\n"
     ]
    }
   ],
   "source": [
    "# Dummy prediction: Random Forest Classifier\n",
    "# We are predicting just individual tweets\n",
    "\n",
    "models = [[\"Random Forest\", RandomForestClassifier()], [\"Linear SVC\", LinearSVC()], [\"Logistic Reg\", LogisticRegression()]]\n",
    "\n",
    "for sel_model in models:\n",
    "    model_name, model = sel_model\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    df_test[\"predictions\"] = prediction\n",
    "    calculate_accuracy_user(model_name)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating new feature: nÂº times a city (in CA) is mentioned by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load [Name, Coordinates, Radius] of each Autonomous Community]\n",
    "Andalusia = [\"Andalusia\", \"37.3399964,-4.5811614,250km\"]\n",
    "Madrid = [\"Madrid\", \"40.5248319,-3.7715628,60km\"]\n",
    "Catalonia = [\"Catalonia\", \"41.8523094,1.5745043,150km\"]\n",
    "Canary_Islands = [\"Canary_Islands\", \"28.5306525,-15.7464439,400km\"]\n",
    "Basque_Country = [\"Basque_Country\", \"42.9911816,-2.5543023,100km\"]\n",
    "\n",
    "CAS = [Andalusia, Madrid, Catalonia, Basque_Country, Canary_Islands]\n",
    "\n",
    "CAS_name = [CA[0].replace(\"_\", \" \") for CA in CAS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe with all unique ids and 0 in each of the CAS\n",
    "df_CA_mentioned = pd.DataFrame(0, index=ids, columns=CAS_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read tweets not processed\n",
    "df_tweets = pd.read_csv(\"tweet_dataset_ext.csv\")\n",
    "\n",
    "# Read cities dataset\n",
    "df_cities = pd.read_csv('cities.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count mentiones of cities for each user in every tweet\n",
    "ids_CA_mention_count = []\n",
    "for id_n in ids:\n",
    "    CA_mention_count = [0,0,0,0,0]\n",
    "    for index, row in df_tweets[df_tweets[\"id\"] == id_n].iterrows():\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[0]][\"city\"])):\n",
    "            CA_mention_count[0] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[1]][\"city\"])):\n",
    "            CA_mention_count[1] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[2]][\"city\"])):\n",
    "            CA_mention_count[2] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[3]][\"city\"])):\n",
    "            CA_mention_count[3] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[4]][\"city\"])):\n",
    "            CA_mention_count[4] += 1\n",
    "            \n",
    "    ids_CA_mention_count.append([id_n, CA_mention_count, row[\"location\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe as csv\n",
    "# Transform to dataframe\n",
    "df_ids_CA_mention_count = pd.DataFrame(ids_CA_mention_count, columns = [\"id\", \"CA_mention_count\", \"location\"])\n",
    "#df_ids_CA_mention_count.to_csv(\"ids_CA_mention_count.csv\", encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Andalusia:\n",
      "    Andalusia counts 89\n",
      "    Madrid counts 41\n",
      "    Catalonia counts 13\n",
      "    Basque_Country counts 1\n",
      "    Canary_Islands counts 0\n",
      "\n",
      "For Madrid:\n",
      "    Andalusia counts 12\n",
      "    Madrid counts 142\n",
      "    Catalonia counts 17\n",
      "    Basque_Country counts 1\n",
      "    Canary_Islands counts 0\n",
      "\n",
      "For Catalonia:\n",
      "    Andalusia counts 16\n",
      "    Madrid counts 47\n",
      "    Catalonia counts 133\n",
      "    Basque_Country counts 2\n",
      "    Canary_Islands counts 0\n",
      "\n",
      "For Basque_Country:\n",
      "    Andalusia counts 18\n",
      "    Madrid counts 87\n",
      "    Catalonia counts 15\n",
      "    Basque_Country counts 44\n",
      "    Canary_Islands counts 0\n",
      "\n",
      "For Canary_Islands:\n",
      "    Andalusia counts 1\n",
      "    Madrid counts 52\n",
      "    Catalonia counts 17\n",
      "    Basque_Country counts 4\n",
      "    Canary_Islands counts 0\n"
     ]
    }
   ],
   "source": [
    "for CA in CAS:\n",
    "    count = [0,0,0,0,0]\n",
    "    for index, row in df_ids_CA_mention_count[df_ids_CA_mention_count[\"location\"] == CA[0]].iterrows():\n",
    "        count = np.add(count, row[\"CA_mention_count\"])\n",
    "    print(\"\\nFor {}:\".format(CA[0]))\n",
    "    for CA_print, count_print in zip(CAS, count):\n",
    "        print(\"    {} counts {}\".format(CA_print[0],count_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
