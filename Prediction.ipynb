{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import math\n",
    "import statistics \n",
    "from statistics import mode \n",
    "\n",
    "# NLP\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweet(sentence):\n",
    "    \"\"\" Process sentece by deleting accents, lowercasing, stemming, \n",
    "    deleting stopwords and cleaning not useful text\"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = unidecode.unidecode(sentence)\n",
    "    words = sentence.split()\n",
    "    if len(words) > 1:\n",
    "        if words[0].lower() == \"rt\" and \"@\" in words[1]:\n",
    "            del words[1]\n",
    "    sentence = \" \".join(words)\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    words = [w for w in words if not w in stopwords.words(\"spanish\")]\n",
    "    words = [w for w in words if (\"rt\" != w.lower()) if (not \"http\" in w)]     # Delete words that contain mentions or the RT word \n",
    "    words = [stemmer.stem(w) for w in words] # Lemmatization spanish\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df_tweets= pd.read_csv(\"tweet_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CAs\n",
    "Andalusia = [\"Andalusia\", \"37.3399964,-4.5811614,250km\"]\n",
    "Madrid = [\"Madrid\", \"40.5248319,-3.7715628,60km\"]\n",
    "Catalonia = [\"Catalonia\", \"41.8523094,1.5745043,150km\"]\n",
    "Canary_Islands = [\"Canary_Islands\", \"28.5306525,-15.7464439,400km\"]\n",
    "Basque_Country = [\"Basque_Country\", \"42.9911816,-2.5543023,100km\"]\n",
    "#Extremadura = \n",
    "#CastillaLaMancha = \n",
    "#CastillaLeon = \n",
    "#Cantabria = \n",
    "#ComunidadValenciana = \n",
    "#Aragon = \n",
    "#LaRioja = \n",
    "#Navarra = \n",
    "#Asturias = \n",
    "#Murcia = \n",
    "\n",
    "CAS = [Andalusia, Madrid, Catalonia, Basque_Country, Canary_Islands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets from dataset\n",
    "processed_dataset = []\n",
    "n_rows = len(df_tweets[\"location\"])\n",
    "actual_row = 1\n",
    "for CA in CAS:\n",
    "    for index, row in df_tweets[df_tweets[\"location\"] == CA[0]].iterrows(): \n",
    "        sentence = process_tweet(row[1])\n",
    "        processed_dataset.append([row[0], sentence, row[2]])   \n",
    "        if actual_row%100 == 0:\n",
    "            print(\"Row {} of {}\".format(actual_row, n_rows))\n",
    "        actual_row += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform to dataframe\n",
    "df_processed = pd.DataFrame(processed_dataset, columns = [\"id\", \"tweet\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe as csv\n",
    "df_processed.to_csv(\"tweet_dataset_processed.csv\", encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv(\"tweet_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3187391489</td>\n",
       "      <td>9 manan cojon dia</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3187391489</td>\n",
       "      <td>unic mied futur pas amistad</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3187391489</td>\n",
       "      <td>i can t stop watching this specific tiktok t c...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3187391489</td>\n",
       "      <td>oye dibuj bien quier hac plan cn algui q guach...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3187391489</td>\n",
       "      <td>punt apart</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              tweet   location\n",
       "0  3187391489                                  9 manan cojon dia  Andalusia\n",
       "1  3187391489                        unic mied futur pas amistad  Andalusia\n",
       "2  3187391489  i can t stop watching this specific tiktok t c...  Andalusia\n",
       "3  3187391489  oye dibuj bien quier hac plan cn algui q guach...  Andalusia\n",
       "4  3187391489                                         punt apart  Andalusia"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all unique ids\n",
    "df_processed = df_processed.dropna()\n",
    "ids = list(df_processed[\"id\"].unique())\n",
    "ids_shuffled = random.sample(ids, len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "# Split ids in train/test 0.9/0.1 approximately\n",
    "print(len(df_processed[\"id\"].unique()))\n",
    "ids_train = ids_shuffled[:228]\n",
    "ids_test = ids_shuffled[228:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test\n",
    "df_train = df_processed[df_processed['id'].isin(ids_train)]\n",
    "df_test = df_processed[df_processed['id'].isin(ids_test)]\n",
    "\n",
    "X_train = df_train[\"tweet\"]\n",
    "X_test = df_test[\"tweet\"]\n",
    "y_train = df_train[\"location\"]\n",
    "y_test = df_test[\"location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform tweets with TFIDF vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3126923076923077\n",
      "Confusion matrix:  [[388 260 113  30 309]\n",
      " [106 112  54  20 108]\n",
      " [151 112  92  24 121]\n",
      " [  0   0   0   0   0]\n",
      " [173 143  47  16 221]]\n"
     ]
    }
   ],
   "source": [
    "# Dummy prediction: Checking accuracy and confusion matrix with Random Forest Classifier\n",
    "# We are predicting just individual tweets\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "#accuracy = metrics.accuracy_score(prediction,y_test)\n",
    "#print(\"Accuracy: \", accuracy)\n",
    "#print(\"Confusion matrix: \", confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction rate 0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for id_num in ids_test:\n",
    "    pred = mode(df_test[df_test[\"id\"] == id_num][\"predictions\"])\n",
    "    real = mode(df_test[df_test[\"id\"] == id_num][\"location\"])\n",
    "    if real == pred:\n",
    "        correct_pred = correct_pred + 1\n",
    "\n",
    "print(\"Correct prediction rate {}\".format(correct_pred/len(ids_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
