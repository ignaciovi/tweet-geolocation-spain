{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "from statistics import mode \n",
    "import itertools\n",
    "\n",
    "# TFIDF\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy_user(model_name):\n",
    "    correct_pred = 0\n",
    "    for id_num in ids_test:\n",
    "        c = Counter(df_test[df_test[\"id\"] == id_num][\"predictions\"])  \n",
    "        mode_count = max(c.values())\n",
    "        mode = {key for key, count in c.items() if count == mode_count}\n",
    "        pred = next(x for x in df_test[df_test[\"id\"] == id_num][\"predictions\"] if x in mode)\n",
    "        #real = mode(df_test[df_test[\"id\"] == id_num][\"location\"])\n",
    "        lst  =(list(df_test[df_test[\"id\"] == id_num][\"location\"]))\n",
    "        real = max(set(lst), key=lst.count)\n",
    "        if real == pred:\n",
    "            correct_pred = correct_pred + 1\n",
    "\n",
    "    print(\"Accuracy for {}: {}\".format(model_name, correct_pred/len(ids_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv(\"tweet_dataset_processed_ext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>625707906</td>\n",
       "      <td>dia ayer amig cit emple call av irigoy 1334 av...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625707906</td>\n",
       "      <td>shaval shaval voy bedford ano vien t co aoxmpm...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625707906</td>\n",
       "      <td>hac falt pens sab llev segund fot pulser lumin...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>625707906</td>\n",
       "      <td>imagin ser asi ridicul pens tod luc movil cans...</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>625707906</td>\n",
       "      <td>k recuerd d bibi limit t co ozagbzlngw</td>\n",
       "      <td>Andalusia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet   location\n",
       "0  625707906  dia ayer amig cit emple call av irigoy 1334 av...  Andalusia\n",
       "1  625707906  shaval shaval voy bedford ano vien t co aoxmpm...  Andalusia\n",
       "2  625707906  hac falt pens sab llev segund fot pulser lumin...  Andalusia\n",
       "3  625707906  imagin ser asi ridicul pens tod luc movil cans...  Andalusia\n",
       "4  625707906             k recuerd d bibi limit t co ozagbzlngw  Andalusia"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle all unique ids\n",
    "df_processed = df_processed.dropna()\n",
    "ids = list(df_processed[\"id\"].unique())\n",
    "ids_shuffled = random.sample(ids, len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553\n"
     ]
    }
   ],
   "source": [
    "# Split ids in train/test 0.9/0.1 approximately\n",
    "# We should predict multiple times with randomly shuffled data to avoid overfitting\n",
    "print(len(df_processed[\"id\"].unique()))\n",
    "ids_train = ids_shuffled[:500]\n",
    "ids_test = ids_shuffled[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train and test\n",
    "df_train = df_processed[df_processed['id'].isin(ids_train)]\n",
    "df_test = df_processed[df_processed['id'].isin(ids_test)]\n",
    "\n",
    "X_train = df_train[\"tweet\"]\n",
    "X_test = df_test[\"tweet\"]\n",
    "y_train = df_train[\"location\"]\n",
    "y_test = df_test[\"location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform tweets with TFIDF vectorizer\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy prediction: Random Forest Classifier\n",
    "# We are predicting just individual tweets\n",
    "\n",
    "models = [[\"Random Forest\", RandomForestClassifier()], [\"Linear SVC\", LinearSVC()], [\"Logistic Reg\", LogisticRegression()]]\n",
    "\n",
    "for sel_model in models:\n",
    "    model_name, model = sel_model\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    df_test[\"predictions\"] = prediction\n",
    "    calculate_accuracy_user(model_name)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For dummy classification, considering only the text tweet as input we get barely a 38% of accuracy\n",
    "# To try to improve on this, we will consider new features to the classification\n",
    "# The number of times a user mention one of the cities in our least, we consider this can be useful for the prediction\n",
    "# Therefore, we will proceed to assign to each user the number of times he mention a city in a CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating new feature: nÂº times a city (in CA) is mentioned by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load [Name, Coordinates, Radius] of each Autonomous Community]\n",
    "Andalusia = [\"Andalusia\", \"37.3399964,-4.5811614,250km\"]\n",
    "Madrid = [\"Madrid\", \"40.5248319,-3.7715628,60km\"]\n",
    "Catalonia = [\"Catalonia\", \"41.8523094,1.5745043,150km\"]\n",
    "Canary_Islands = [\"Canary_Islands\", \"28.5306525,-15.7464439,400km\"]\n",
    "Basque_Country = [\"Basque_Country\", \"42.9911816,-2.5543023,100km\"]\n",
    "\n",
    "# Other CAs not used for now\n",
    "#Extremadura = \n",
    "#CastillaLaMancha = \n",
    "#CastillaLeon = \n",
    "#Cantabria = \n",
    "#ComunidadValenciana = \n",
    "#Aragon = \n",
    "#LaRioja = \n",
    "#Navarra = \n",
    "#Asturias = \n",
    "#Murcia = \n",
    "\n",
    "CAS = [Andalusia, Madrid, Catalonia, Basque_Country, Canary_Islands]\n",
    "\n",
    "CAS_name = [CA[0].replace(\"_\", \" \") for CA in CAS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe with all unique ids and 0 in each of the CAS\n",
    "df_CA_mentioned = pd.DataFrame(0, index=ids, columns=CAS_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read tweets not processed\n",
    "df_tweets = pd.read_csv(\"tweet_dataset_ext.csv\")\n",
    "\n",
    "# Read cities dataset\n",
    "df_cities = pd.read_csv('cities.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count mentiones of cities for each user in every tweet\n",
    "ids_CA_mention_count = []\n",
    "for id_n in ids:\n",
    "    CA_mention_count = [0,0,0,0,0]\n",
    "    for index, row in df_tweets[df_tweets[\"id\"] == id_n].iterrows():\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[0]][\"city\"])):\n",
    "            CA_mention_count[0] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[1]][\"city\"])):\n",
    "            CA_mention_count[1] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[2]][\"city\"])):\n",
    "            CA_mention_count[2] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[3]][\"city\"])):\n",
    "            CA_mention_count[3] += 1\n",
    "        if any(i in row[\"tweet\"].split() for i in list(df_cities[df_cities[\"admin\"] == CAS_name[4]][\"city\"])):\n",
    "            CA_mention_count[4] += 1\n",
    "            \n",
    "    ids_CA_mention_count.append([id_n, CA_mention_count])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as csv\n",
    "# Transform to dataframe\n",
    "df_ids_CA_mention_count = pd.DataFrame(ids_CA_mention_count, columns = [\"id\", \"CA_mention_count\"])\n",
    "df_ids_CA_mention_count.to_csv(\"ids_CA_mention_count.csv\", encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ids_CA_mention_count = pd.read_csv(\"ids_CA_mention_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_CA_count = []\n",
    "for index, row in df_processed.iterrows():\n",
    "    count_selected = [el for el in ids_CA_mention_count if el[0] == row[\"id\"]]\n",
    "    list_with_CA_count.append(list(row) + count_selected[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[625707906, [1, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Transform to dataframe\n",
    "\n",
    "df_processed_with_CA_count = pd.DataFrame(processed_dataset, columns = [\"id\", \"tweet\", \"location\", \"Andalusia_mention\", \"Madrid_mention\", \"Catalonia_mention\", \"Basque_Country_mention\", \"Canary_Islands_mention\"])\n",
    "\n",
    "# Save dataframe as csv\n",
    "df_processed_with_CA_count.to_csv(\"tweets_processed_with_CA_count.csv\", encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_selected[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
